# -*- coding: utf-8 -*-
"""Neural Network Traffic Solutions

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gke7hVyRODfCOCkTbqbo0No47vvsr_Dj
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

#Creating our model class

class Model(nn.Module):
  # Input layer (4 features) -> hidden layer1 (some neuron count) -> hidden layer2 (neurons) -> output (3 classes)
  def __init__(self, in_features=20, h1 = 40, h2=40, h3=40, h4 =20, out_features=4):
    super().__init__()
    self.fc1 = nn.Linear(in_features, h1)
    self.fc2 = nn.Linear(h1, h2)
    self.dropout = nn.Dropout(p=0.1)
    self.fc3 = nn.Linear(h2, h3)
    self.out = nn.Linear(h3, out_features)

  def forward(self, x):
    x = F.tanh(self.fc1(x))
    x = self.dropout(x)
    x = F.tanh(self.fc2(x))
    x = F.tanh(self.fc3(x))
    x = self.dropout(x)
    x = self.out(x)

    return x

#manual seed for randomization
torch.manual_seed(42)

model = Model()

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import pandas as pd

# %matploylib inline

my_df = pd.read_csv("combine.csv")
my_df = my_df.drop(["Group", "ID"], axis = 1)
my_df

#change to numerical
my_df["Result"] = my_df["Result"].replace("North/South Lights Green", 0.0)
my_df["Result"] = my_df["Result"].replace("East/West Lights Green", 1.0)
my_df["Result"] = my_df["Result"].replace("East/West Left Turns Green", 2.0)
my_df["Result"] = my_df["Result"].replace("North/South Left Turns Green", 3.0)
my_df.columns

# train test split

x = my_df.drop("Result", axis = 1)
y = my_df["Result"]

# convert to numpy arrays

X = x.values
Y = y.values

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.20, random_state = 42)

# convert to tensors (X)
X_train = torch.FloatTensor(X_train)
X_test = torch.FloatTensor(X_test)

# convert to tensors (Y)
Y_train = torch.LongTensor(Y_train)
Y_test = torch.LongTensor(Y_test)

# set criterion of model to measure error
criterion = nn.CrossEntropyLoss()
# Choose Adam Optimizer, lr = learning rate (if error doesnt go down after epochs, lower the learning rate)
optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)

# TRAINING

epochs = 500
losses = []

for i in range(epochs):
  Y_pred = model.forward(X_train) # Get prediction

  #measure the loss/error
  loss = criterion(Y_pred, Y_train) #predicted vs real

  #keep track of loses

  losses.append(loss.detach().numpy())

  #print every 10 epochs
  if i % 10 == 0:
    print(f"Epoch: {i}\n Loss: {loss}")

  #back propogation to fine tune the model
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

plt.plot(range(epochs), losses)

# Evaluate prediction on test set
with torch.no_grad():
  Y_eval = model.forward(X_test)
  loss = criterion(Y_eval, Y_test)

loss

correct = 0
wrong = 0

with torch.no_grad():
  for i, data in enumerate(X_test):
    Y_val = model.forward(data)

    if Y_val.argmax().item() == Y_test[i]:
      correct += 1
    else:
      wrong += 1

  print(correct/(correct + wrong) * 100)

new_test = torch.tensor([ 0.0, 0.0, 0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,10.0,0.0, 0.0,0.0,0.0,0.0,0.0,0.0])

with torch.no_grad():
  print(model.forward(new_test).argmax().item())

# prompt: save torch model

torch.save(model.state_dict(), 'traffic_model.pt')
